{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52651355-8a61-4a92-b874-34de0d780906",
   "metadata": {},
   "source": [
    "# Flight Streaming Data\n",
    "\n",
    "This directory has code and schemas for interacting with the streaming data from the [OpenSky Network](https://opensky-network.org).\n",
    "\n",
    "The python/flight/stream folder contains code that will pull the latest data from OpenSky and store the data in GCS and/or publish the data as messages in a Pub/Sub queue. The documentation for the OpenSky API is found at [https://openskynetwork.github.io/opensky-api/](https://openskynetwork.github.io/opensky-api/).\n",
    "\n",
    "## How to Run Serveless\n",
    "\n",
    "To run the code using a Cloud Function (aka. using serverless computing) you:\n",
    "\n",
    "1. Configure your cloud function with permissions, time limits, and memory limits.\n",
    "2. Supply the code and requirements.txt to the function. *This applies to a Cloud Function based on Python.*\n",
    "3. Deploy the function.\n",
    "4. Test the function with an example message.\n",
    "\n",
    "You can then create a Big Query table from either the Cloud Storage bucket or the Pub/Sub queue using the provided schemas.\n",
    "\n",
    "### Configure a Cloud Function\n",
    "\n",
    "1. Environment: You can use either 1st Gen or 2nd Gen. 1st Gen has an easier interface for testing. 2nd Gen has the ability to have longer running cloud functions. (1st Gen will be limited to 10 mins.)\n",
    "2. Function name: Choose a meaningful name. It can only have lowercase characters, numbers, and hyphens.\n",
    "3. Trigger: Select HTTP/HTTPS. This means the function will be triggered whenever anyone hits a specific URL. (Other triggers are possible, such as triggering a function whenever a file is uploaded to cloud storage, a message is published in a Pub/Sub queue, and so on.)\n",
    "4. Authentication: Allow unauthenticated invocations. Permissions often get hairy to set up properly, though you will want to enforce security constraints when working in a production system. For our purposes, we will not restrict access to the URL that is used to trigger the Cloud Function.\n",
    "5. *(If using 1st Gen)* Click SAVE at the bottom of the Trigger box.\n",
    "6. Runtime, build, and connection settings. You don't need to change the defaults. You may want to change the following: Memory allocated (if your function generates \"Out of Memory\" errors, but smaller memory allocation relates to smaller costs); Timeout (increase up to the maximum if your function fails to complete in time; smaller timeout means a function that hangs will be terminated more quickly); Autoscaling (lower the Maximum number of instances if you want to make sure the function doesn't spawn thousands of instances at one time when you have a lot of trigger events).\n",
    "\n",
    "### Supply the Code\n",
    "\n",
    "1. Select one of the Python 3 versions.\n",
    "2. The entry point of the code is a method named flightStreaming. Change the default entry point from hello_world to flightStreaming.\n",
    "3. Package up the code.\n",
    "\n",
    "  A script is provided to package up the code for the flight streaming function. It creates a folder in /tmp/flight-streaming_zip and puts the subset of python files there, zips them up, and stores the zip file in Google Cloud Storage in a function subfolder in your bucket.\n",
    "\n",
    "  You provide the script with the name of the cloud function, which is flight-streaming in this case.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5057f903-d260-4a3e-8aba-4f99aea7fdf0",
   "metadata": {},
   "source": [
    "sh/createFlightStreamingZip.sh flight-streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1edcae-c1fd-42cc-8071-43c33226d1f1",
   "metadata": {},
   "source": [
    "You can browse Cloud Storage in the Cloud Function console to select the zip file.\n",
    "\n",
    "You should see something like:\n",
    "\n",
    "![Folder structure for Flight Streaming function](img/flightStreamingFolderStruct.png)\n",
    "\n",
    "Once ready, select DEPLOY. It will take some time to create a virtual machine image and deploy your code into it. It will finish spinning and either have a green check mark or an error.\n",
    "\n",
    "### Test Your Function\n",
    "\n",
    "You can manually trigger your function using the Testing tab in the Cloud Function console (for 1st Gen) or copy-pasting a curl command in Cloud Shell (for 2nd Gen). In either case, you can supply a test such as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf175d79-6b4f-43e4-a924-8b3b57072469",
   "metadata": {},
   "source": [
    "{\"storage\":true,\"bucket\":\"prof-big-data_data\",\"path\":\"flight-streaming\",\"separateLines\":true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f9a34-a78d-4611-92d0-5d0856fd87fd",
   "metadata": {},
   "source": [
    "See the openskyParser.py file for more examples.\n",
    "\n",
    "## How to Run Locally *(for testing)*\n",
    "\n",
    "To execute the Python code, you will need to follow the instructions in the README (displayed in the main page in the GitHub repository.) You only need to set up your Python environment if you want to run the code from Cloud Shell (to test it out without using Cloud Function.)\n",
    "\n",
    "## Schemas\n",
    "\n",
    "The schema folder has schemas used both for BigQuery tables as well as for connecting a Pub/Sub topic to a BigQuery table.\n",
    "BigQuery schemas are used when you create a table and do not select to autodetect the schema. \n",
    "\n",
    "To create a table as an endpoint for messages published in a Pub/Sub topic will also need a schema that defines how to create columns out of the contents of the messages. The schema needed by DataFlow uses different types names than BigQuery.\n",
    "\n",
    "**schema/openSky_bigQuery.json** -- a schema that can be used when loading data from Cloud Storage.\n",
    "\n",
    "**schema/openSky_dataFlow.json** -- a schema that defines how to map the fields in a Pub/Sub message to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3fbdf-9ff4-45d8-9732-e29c4eb9a31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_venv",
   "language": "python",
   "name": "bigdata_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
